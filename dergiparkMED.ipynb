{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO168mNk6fZt75ovN6tcrjh"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Installations and Imports"
      ],
      "metadata": {
        "id": "eKjIm-YnXjWM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "as3Y-6OfXfin",
        "outputId": "760dda85-b9cd-4d29-bfa6-fc1f11f8eddc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.8/125.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "! pip install requests -q\n",
        "! pip install html5lib -q\n",
        "! pip install bs4 -q\n",
        "! pip install pdfminer.six -q\n",
        "! pip install tiktoken -q\n",
        "! pip install fake-useragent -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import time\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import chardet\n",
        "from io import BytesIO\n",
        "from pdfminer.pdfparser import PDFParser\n",
        "from pdfminer.pdfdocument import PDFDocument\n",
        "from pdfminer.high_level import extract_text\n",
        "import tiktoken\n",
        "from fake_useragent import UserAgent\n",
        "import copy\n",
        "from typing import List, Dict\n",
        "import random\n",
        "from google.colab import files\n",
        "from threading import Semaphore\n",
        "import re"
      ],
      "metadata": {
        "id": "xk7VjjwEXilS"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting the List of Medical Journals"
      ],
      "metadata": {
        "id": "hgPuXUR9Xuh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_url = \"https://turkmedline.net/dergi-listesi\"\n",
        "params = {\n",
        "    \"searchkw\": \"\",\n",
        "    \"action\": \"search\"\n",
        "}"
      ],
      "metadata": {
        "id": "wi0k9WvBYC2a"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = requests.get(base_url, params=params)\n",
        "soup = BeautifulSoup(res.content, \"xml\")"
      ],
      "metadata": {
        "id": "L-XyHqwuYeGP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = soup.find_all(class_ = \"text-decoration-none indeks-dergi-link\")"
      ],
      "metadata": {
        "id": "Ai2AEtbcYv6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "links = []\n",
        "for cls in classes:\n",
        "  link = cls.get(\"href\")\n",
        "  link = link.replace(\"dergi-listesi\", \"\")\n",
        "  links.append(link)"
      ],
      "metadata": {
        "id": "oKZqXshOZ8W8"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dergipark_sets(base_url, links):\n",
        "  sets = []\n",
        "  for link in links:\n",
        "    try:\n",
        "      full_url = base_url + link\n",
        "      res = requests.get(full_url)\n",
        "      soup = BeautifulSoup(res.content, \"html\")\n",
        "      href = soup.find(\"a\", href=re.compile(\"dergipark\"))\n",
        "      href = href.get(\"href\")   # parse\n",
        "      sets.append(href)\n",
        "    except:\n",
        "      pass\n",
        "  return sets\n",
        "\n",
        "sets = get_dergipark_sets(base_url, links)"
      ],
      "metadata": {
        "id": "eDhvdVcfhKXm"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_set_names(sets):\n",
        "  set_names = []\n",
        "  for s in sets:\n",
        "    res = s.split(\"/\")\n",
        "    set_name = res[-1]   # last element is the dergipark set_name\n",
        "    set_names.append(set_name)\n",
        "  return set_names\n",
        "\n",
        "set_names = get_set_names(sets)"
      ],
      "metadata": {
        "id": "yvu1op_CilEV"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json_obj = json.dumps(set_names)\n",
        "with open(\"med_set_names.json\", \"w\") as outfile:\n",
        "  outfile.write(json_obj)"
      ],
      "metadata": {
        "id": "COa04ouHuRZJ"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fetch Article Links"
      ],
      "metadata": {
        "id": "MtFfflwaxswy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tur_records = []\n",
        "\n",
        "def fetch_record(set_name):\n",
        "    base_url = \"https://dergipark.org.tr/api/public/oai/\"\n",
        "    params = {\n",
        "        \"verb\": \"ListRecords\",\n",
        "        \"metadataPrefix\": \"oai_dc\",\n",
        "        \"set\": set_name\n",
        "    }\n",
        "\n",
        "    with requests.Session() as session:\n",
        "        session.headers.update({'User-agent': 'your bot 0.1'})\n",
        "        count = 0\n",
        "        while True:\n",
        "            try:\n",
        "                res = session.get(base_url, params=params, timeout=10) # timeout is to prevent error 429: too many requests\n",
        "                res.raise_for_status()\n",
        "                soup = BeautifulSoup(res.content, \"xml\")\n",
        "\n",
        "                records = soup.find_all(\"record\")\n",
        "                for record in records:\n",
        "                    lang = record.find(\"dc:language\")\n",
        "                    if lang and lang.text == \"tur\":\n",
        "                        relation = record.find(\"dc:relation\")\n",
        "                        identifier = record.find(\"dc:identifier\")\n",
        "                        title = record.find(\"dc:title\")\n",
        "                        date = record.find(\"dc:date\")\n",
        "                        year = date.text.strip().split(\"-\")[0] if date else None\n",
        "                        year = int(year) if year else 0\n",
        "                        if (year>=2020 and relation and title):\n",
        "                          rec = {\n",
        "                                \"set_name\": set_name,\n",
        "                                \"year\": year,\n",
        "                                \"title\": title.text.strip(),\n",
        "                                \"dergipark_url\": identifier.text.strip() if identifier else None,\n",
        "                                \"pdf_url\": relation.text.strip(),\n",
        "                            }\n",
        "                          tur_records.append(rec)\n",
        "                          count += 1\n",
        "                resumption_token = soup.find(\"resumptionToken\")\n",
        "                if resumption_token and resumption_token.text.strip():\n",
        "                    params = {\n",
        "                        \"verb\": \"ListRecords\",\n",
        "                        \"resumptionToken\": resumption_token.text.strip()\n",
        "                    }\n",
        "                else:\n",
        "                    break\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                print(f\"Error fetching set {set_name}: {e}\")\n",
        "                time.sleep(5)  # retry after a delay\n",
        "                continue\n",
        "\n",
        "    print(f\"Finished fetching journal #{set_names.index(set_name)}: {set_name}\")"
      ],
      "metadata": {
        "id": "WzJNuDQuxOHh"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for st in set_names:\n",
        "  fetch_record(st)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "P-RS1Ft2yHin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame.from_records(tur_records, index=None)\n",
        "df.to_pickle(\"tur_records_links.pkl\")"
      ],
      "metadata": {
        "id": "CXOssuu17FLJ"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scraping the Text from Records"
      ],
      "metadata": {
        "id": "n19BswYiF--v"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c7ccBar1F-rw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}